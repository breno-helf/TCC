%% ------------------------------------------------------------------------- %%
\chapter{Hash Functions}
\label{cap:Hash Functions}

%% ------------------------------------------------------------------- %%
%\begin{itemize}
%\item Define Hash Function
%\item Give some examples of hash functions (Multiplicative / Remainder)
%\item Present Dragon Book metric and Evaluate hash functions
%\end{itemize}
%% ------------------------------------------------------------------- %%

Outside computer science, the word \textit{``hash''} in the English language means to ``chop'' or to ``mix'' something. This meaning is entirely related to what hash functions are supposed to do. Hash functions are functions that are used to map data of an arbitrary size to data of a fixed size \citep{HashFuncWiki}.

They have wide applications in computer science, being used in information and data security, compilers, distributed systems and hardcore algorithms. In this chapter we first define and explain the basics of a hash function, then we give an intuition on some metrics that tries to capture the idea of what is expected of a good hash function, as discussed in the famous \textit{``Red Dragon Book''} \citep{DragonBook} along with some reproduction of known results in the area.

The value extracted from the hash function for an object or key is usually called \textit{hash value} or simply \textit{value}. The hash value is in general, but not necessarily, smaller than the object that generated it (Figure~\ref{fig:hashfunction}). For example, we can have a hash function that takes Gigabytes or Terabytes files and returns an 8 bytes hash value. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=6cm]{figuras/hash-function.pdf}
  \caption{Illustration of a hash function from string to 4 bit integer. Source: Wikipedia, Jorge Stolfi }
  \label{fig:hashfunction}
\end{figure}

\bigskip

\section{Definition}

A hash function over a set \( X \) is a function \( H \) that takes an element \( x \) in \( X \) and associates to \( x \) an integer \( H(x) \) in the interval \( [0,M) \), for some integer \( M \). In symbols
\[ H: X \rightarrow [0, M) \]
When dealing with hash tables, \( X \) is the set of possible keys and \( M \) is the size of the table, which is usually just an array. Moreover, we saw earlier that hash functions are usually more useful when \( |X| > M \). This is the same definition used by Donald Knuth \citep{TAOCP3} and some articles \citep{RobinHoodHashing}. This definition makes sense for our purpose because we will be talking mostly about hash functions used in hash tables. In that case we want integers that will be indexes in an array (as we will see later on). In other cases we may see hash values as strings, like for when we hash a string for password storage or when we use a hash function in files for checking integrity (for when we are checking if two files are the same). For the goal of this text we will not cover those functions, but it is important to notice that strings can also be easily associated to integers if we just look at their bytes.

For our purpose we are looking for a hash function that performs well for the construction of hash tables. Ideally, these functions should be fast to compute and minimize the number of collisions. Depending on our goals we might want a different metric, for check-sums for example we may want a function that is very sensible to changes, and for passwords one that is very hard to find its inverse, those are so called cryptographic functions. For some collision resolution techniques, as we will see later, we may also want that the hash function disperse the values well too. Intuitively, a hash function should look like a random function having a given key as seed.

As written in Knuth's book, we know that it is theoretically impossible to create a hash function that generates true random data from non random data in actual file, but we can do pretty close to that or in some cases, even fewer collision than an uniformly random function. Knuth describes two specific methods for simple hash function, named \textit{division hashing} and \textit{multiplicative hashing} techniques. As the name suggests, the first is based on division operation and the latter on multiplication operation.

\section{Division and Multiplicative Methods}

The \textit{division hashing} method maps an integer \( X \) associated to the data to its remainder modulo an integer \( M \). Supposing we can represent the data as a non negative integer \( X \), the division hashing would be to choose a value M and the hashing function would be \( X \mod M \). The code would look as following:

\begin{lstlisting}
unsigned int divisionHashing(unsigned int X, unsigned int M) {
  return X % M;
}
\end{lstlisting}

A good hash function combines number theory, statistics and engineering and in general, large prime numbers tend to be a good value to \( M \), to avoid unwanted patterns or repetitions. One great example of this is if \( M \) is even, then the parity of hash value of \( X \) will match the parity of \( X \) (which will cause a bad distribution).

For \textit{multiplicative hashing}, let's first suppose once more that we can represent the data as a non-negative integer \( X \) and we have chosen a constant \( S \), where \( 0 < S < 1 \). Then we multiply \( X \) by \( S \) and extract the fractional part of \( X \times S \), that is \( X \times S \mod  1 \). We can calculate that by doing: \( X \times S - \lfloor X \times S \rfloor \). Then we multiply that value, that will be between 0 and 1, by \( M \). The code of what was described above would look as following:

\medskip

\begin{lstlisting}
unsigned int multiplicativeHashing(unsigned int X,
                                   double S,
                                   unsigned int M) {
  double alpha = (double) X * S - floor((double) X * S);
  return (unsigned int) floor(alpha * (double) M);
}
\end{lstlisting}

In Knuth's book he describes \( S \) as being an integer \( A \) divided by \( w \), where \( w \) is the size of a ``word'' in our computer. He restricts \( A \) to be relatively prime to \( w \). That definition is often useful if one wants to retrieve a value \( Y \) for a given hash value \( F \). This can be done via Bezout theorem \citep{TAOCP3}. It is good to note here that if \( H(X) = F \) and \( H(Y) = F \), \( X \) is not necessarily equal to \( Y \), as two different keys can have the same hash value.

\section{Hashing Strings}

We have many ways of converting non numerical data to non negative integers. In the end, it is all just a sequence of bytes, that when read in a specific way form another type of data, such as images or strings. For example, one way of transforming a string to a non-negative integer is summing the ASCII value of its characters. The C++14 code for that would look as following:

\begin{lstlisting}
unsigned int stringToInteger(string str) {
  unsigned int hashValue = 0;
  for (char c : str) {
    hashValue += (int) c;
  }
  return hashValue;
}
\end{lstlisting}

We always use unsigned integers for our non negative integer calculations due to the natural modulo operation of it on overflow cases. It is equivalent to having a \( \mod \ 2^{32} \) every time it overflows, as we only look at the leading 32 bits. We can also use XOR function to mix numbers together. 

There is also a very common type of hash functions that tend to work pretty well for strings \citep{DragonHashFunc}. We can also think of a ``superset'', of generalization, of multiplicative hash functions. The C++14 code would look as following:

\begin{lstlisting}
unsigned int hashForString(string str,
                           unsigned int initialValue,
                           unsigned int multiplier,
                           unsigned int modulo) {
  unsigned int hash = initialValue;
  for (char c : str) {
    hash = (multiplier * hash + (int)c);
  }
  return hash % modulo;
}
\end{lstlisting}

The above function is very common for string hashing, and by just choosing a different initial value and multiplier we can have completely different hash functions. Although using summing or using XOR to combine the previous hash value with the new character usually don't provide much difference, with XOR operation we do not need to worry about overflow. Some values are of known hash functions, for example with \texttt{multiplier = 33} and \texttt{initialValue = 5381} generates \textit{Bernstein hash djb2} \citep{BernsteinHash} or \texttt{multiplier = 31} and \texttt{initialValue = 0} generates \textit{Kernighan and Ritchie's hash} \citep{KernighanHash}. Those are famous functions and their values are not chosen randomly, as there are some factors that maximize the chance of producing a good hash function, where good means low collision rate and fast computation. Those factors are:

\begin{itemize}
\item The multiplier should be bigger than the size of the alphabet, in our case usually 26 for English words or 256 for ASCII. That is the case because if it is smaller we can have wrong matches easier. For example, suppose that \texttt{multiplier = 10} and \texttt{initialValue = 0}, we have \( H('ABA') = H('AAK') = 7225 \) before taking the modulo operation.

\item The multiplication by the multiplier should be easy to compute with simple operations, such as bitwise operations and adding. That is quite intuitive as we want a hash function that is fast to calculate. 

\item The multiplier should be coprime with the modulo. That is because otherwise we will ``cycle'' hashes at a greater rate than the modulo (We can use some modular arithmetic to prove that). Usually prime numbers tend to be good multipliers.
\end{itemize}

\section{Quality of Hash Functions}

Now that we know some good templates for producing hash functions let us try to find a concrete metric or formula that measures the quality of a hash function. Fortunately, the famous book Compilers: Principles, Techniques, and Tools, also known as ``Red Dragon Book'' \citep{DragonBook}, has already proposed a quantity to measure the quality of a hash function. This quantity is given by:
\[ \mathlarger{\sum}_{j = 0}^{m - 1} \frac{b_j(b_j + 1)/2}{ (n/2m)(n + 2m - 1) }, \]
where \( n \) is the number of keys, \( m \) is the number of total slots and \( b_j \) is the number of keys in the \( j \)-th slot. The intuition for the numerator is that it represents the number of operations we will need to perform to find each element of the table. For example, we need 1 operation to find the first element, 2 to find the second, and so on. That means that we will end up with an arithmetic progression. We know that a hash function that distributes the keys according to an uniformly random distribution has expected bucket size of \( n / m \), so we can calculate that the expected value of the numerator formula is \( (n/2m)(n + 2m - 1) \). So that gives us a ratio of collisions (thinking just about operations to access a value) of ``our'' hash function with an ``ideal'' function. That means that a value close to \( 1.00 \) of the above formula is good, and values below \( 1.00 \) means that we had less collisions than an uniformly distributed random function.

For common data as the ones in Dragon Book and Strchr website \citep{DragonHashFunc} some tests with the previously cited hash functions were performed.

\begin{figure}[h!]
  \centering
  \hspace*{-3cm}
  \includegraphics[width=20cm]{figuras/1031HashFunc.png}
  \caption{Functions tested against a ``small'' table}
\end{figure}

\medskip

\begin{figure}[h!]
  \centering
  \hspace*{-3cm}
  \includegraphics[width=20cm]{figuras/16411HashFunc.png}
  \caption{Functions tested against a ``medium'' sized table}
\end{figure}

\newpage

\begin{figure}[h!]
  \centering
  \hspace*{-2cm}
  \includegraphics[width=20cm]{figuras/1031101HashFunc.png}
  \caption{Functions tested against a ``large'' table}
\end{figure}

The results are shown in the same way displayed in the Red Dragon Book, with hash functions in the \( x \) axis and the collision ratio displayed in the \( y \) axis, with different identification for each file. We consider three sizes of tables to count collisions, a ``small'', ``medium'' and a ``large'' table, the small table having a load factor (The percentage of the table occupied) of approximately \( \sim 0.5 \), the medium with \( \sim 0.05 \) and large with \( \sim 0.005 \). It's assumed that the modulo is not the responsibility of the hash function, so all hash functions return values from 0 to \( 2^{32} - 1 \), and the modulo is taken depending on the size of the table. From that we can already see that the load factor doesn't make a good hash function bad, but expose problems of ``bad'' hash functions in some cases. 

Another fact that it is important to notice from the graph is the red dotted lines. The top one is the ``Upper'' threshold, which results greater than 6 are just considered ``Big'', as in some cases the ratio exploded to values up to 200. The lower 2 red lines are in \( y = 1.05 \) and \( y = 0.95 \) which is the interval that we consider a hash function to have ``Good'' values. 

The tests were made with 10 different hash functions, tested against 9 different files (which can be found in strchr website \citep{DragonHashFunc}). All of the code used to test this can be found in the github repo \citep{GithubRepo}. The 10 hash functions are the following:

\begin{itemize}
\item \textbf{bernsteinHashADD}: The Bernstein hash function described earlier. We use the hash template adding the elements showed earlier. The multiplier is 33 and the initial value is 5381. In the end we XOR the bits of the hash with itself shifted 16 to the right (That is half of the bits with our implementation).
  
\item \textbf{bernsteinHashXOR}: The same as above but substituting the first adding operator by the XOR operation.

\item \textbf{kernighanRitchieHashADD} The Kernighan and Ritchie Hash function described earlier. We use the given hash template adding the elements. The multiplier is 31 and the initial value is 0.

\item \textbf{kernighanRitchieHashXOR} The same as above but substituting the first adding operator by the XOR operation.

\item \textbf{redDragonBookHash} The hash function tested in the Red Dragon Book. It is described as x65599 in the book.

\item \textbf{defaultHash} The default hash function of C++ standard template library.

\item \textbf{paulHesiehHash} A fast hash function described by Paul Hesieh \citep{PaulHesiehHash}. It is fast to calculate and more complex than Knuth multiplicative or division Hashing.
  
\item \textbf{dumbHashADD} A hash function that simply add all characters.

\item \textbf{dumbHashXOR} A hash function that simply XOR all characters.

\item \textbf{identityHash} A hash function that takes the first 4 bytes of the string.  
\end{itemize}

We have a variety of hash functions, with all being considered ``fast'' hash functions. The files tested include common words in English and french, strings of some IP values, numbers, common variable names and words with common prefix and suffix.

First, we can notice from those graphs that changing the ADD function to XOR doesn't make a good multiplicative hash function bad. Both are actually ``Equivalent'' given that we are also multiplying the values. For \texttt{dumbHashADD} and \texttt{dumbHashXOR} we can see clear differences, with \texttt{dumbHashXOR} being clearly worse. This can be explained by the cancellation property of XOR. We can see this example on the hash of this IP below:

\texttt{dumbHashXOR('168.1.1.0') = dumbHashXOR('168.2.2.0') = dumbHashXOR('124.6.8.0')}

We can see that many different IPs have the same hash value. More than that, XOR don't increase the number of bits, so all the hashes will be of just 1 byte.

We can also notice that ``identity'' hash is good or perfect in some cases. One obvious case that ``identity'' function works perfectly is for numbers as we will have 0 collisions. Some languages, like Python 3, use the identity function to calculate hash for integers, as it is very fast and produces no collisions. But we can see that for other cases, such as common prefix, it works terrible as we just get the first 4 bytes.

The most common multiplicative hash functions tend to work similarly well, being reasonably close to an uniformly random function (that is our ``ideal'' hash function) in all cases.

As we can see, we don't need a lot of complexity to make a good hash function for a hash table. We have some functions working better for some specific case, like identity function working well for numbers, but general functions already work well enough.

It is important to note here that hash function is a very vast topic, and here we just covered hash functions related to hash tables. Hash functions have applications in distributed systems (consistent hashing), database indexing, caching, compilers (Red Dragon Book) and cryptography. Each application has different requirements and make some hash functions better than others.